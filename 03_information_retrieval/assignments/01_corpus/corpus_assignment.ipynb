{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Assigment 2\n",
    "Group 3: Hooshyar Hosna, Lima Rachel, Lorefice Alessandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from numpy import cumsum\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read textual documents from file\n",
    "documents_path = 'AssociatedPress.txt'\n",
    "with open(documents_path, 'r', encoding='utf-8') as doc_f:\n",
    "    corpus_list = doc_f.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokens(collection):\n",
    "    \n",
    "    '''\n",
    "    given a collection as input, it gives a list of all the tokens of that collection\n",
    "    and a list lists where each list contains the tokens for that document of the collection\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #create create a list of lists for the tokens of each document\n",
    "    tokenized_collection = []\n",
    "\n",
    "    for doc in collection:\n",
    "        tokenized_collection.append(nltk.tokenize.word_tokenize(doc))\n",
    "        \n",
    "        \n",
    "    #create a list of the total tokens\n",
    "    tokens = []\n",
    "\n",
    "    for doc_tokens in tokenized_collection:\n",
    "        for word in doc_tokens:\n",
    "            tokens.append(word)\n",
    "            \n",
    "            \n",
    "    return tokens, tokenized_collection;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinctTerms(tokenized_collection, tokens):\n",
    "    \n",
    "    '''\n",
    "    given a tokenized collection and the total tokens as input, it gives a list of all \n",
    "    the distinct terms of that collection and a list lists where each list contains \n",
    "    the distinct terms for that document of the collection\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #create create a list of lists for the distinct terms of each document\n",
    "    distinct_terms_collection = []\n",
    "\n",
    "    for doc_tokens in tokenized_collection:\n",
    "        distinct_terms_collection.append(sorted(set(doc_tokens)))\n",
    "        \n",
    "        \n",
    "    #create a list of the total distinct terms\n",
    "    distinct_terms = list(set(tokens))\n",
    "            \n",
    "            \n",
    "    return distinct_terms, distinct_terms_collection;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    if \",\" in s:\n",
    "        s = s.replace(\",\",\"\")\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TermsNoNum(terms):\n",
    "    \n",
    "    '''\n",
    "    given as input a list of terms, it gives a list of terms without numbers\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    terms_no_number = []\n",
    "\n",
    "    for term in terms:\n",
    "        if not is_number(term):\n",
    "            terms_no_number.append(term) \n",
    "            \n",
    "    return terms_no_number;\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepOnlyAlph(terms):\n",
    "    \n",
    "    '''\n",
    "    given a list of strings, it checks if the strings are composed just by\n",
    "    alphabetical values and returns a list containing only those strings \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    only_alpha_value_terms = []\n",
    "    \n",
    "    for term in terms:\n",
    "        if term.isalpha() or (\"-\" in term and re.search(\"[a-zA-Z]\", term.replace(\"-\",\"\"))):\n",
    "            only_alpha_value_terms.append(term)\n",
    "                   \n",
    "    return only_alpha_value_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseFolding(terms):\n",
    "    \n",
    "    '''\n",
    "    given as input a list of terms, it gives a list of terms without uppercases\n",
    "    \n",
    "    '''\n",
    "\n",
    "    terms_no_uppercase = []\n",
    "    \n",
    "    for term in terms:\n",
    "        if term.isalpha():\n",
    "            terms_no_uppercase.append(term.lower()) \n",
    "        else:\n",
    "            terms_no_uppercase.append(term)\n",
    "            \n",
    "    return terms_no_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(terms, n):\n",
    "    \n",
    "    '''\n",
    "    given as input a list of terms and the number of stop words to be removed, \n",
    "    it gives a list of terms removing those stop words stop words\n",
    "    if the number of stop words is bigger than the total number of stop words we have\n",
    "    it will remove the maximum number of stop words possible\n",
    "    \n",
    "    '''\n",
    "\n",
    "    terms_no_stopwords = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    if n >= len(stop_words):\n",
    "        n = len(stop_words)\n",
    "    \n",
    "    for term in terms:\n",
    "        if term not in stop_words[0:n]:\n",
    "            terms_no_stopwords.append(term)\n",
    "            \n",
    "    return terms_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(terms):\n",
    "    \n",
    "    '''\n",
    "    given as input a list of terms, it gives a list of terms after stemming\n",
    "    we can have repetitions\n",
    "    \n",
    "    '''\n",
    "\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    terms_stemmed = []\n",
    "    \n",
    "    for term in terms:\n",
    "        terms_stemmed.append(ps.stem(term))\n",
    "\n",
    "    return terms_stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(terms):\n",
    "    \n",
    "    '''\n",
    "    given as input a list of tokens, it computes the frequency of a token in that list\n",
    "    \n",
    "    '''    \n",
    "\n",
    "    return nltk.FreqDist(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reductionPercentage(before_red, after_red):\n",
    "    \n",
    "    '''\n",
    "    takes as input 2 ints that represent a value before a given reduction and a value\n",
    "    after a reduction has been applied and compute the percentange of that reduction\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    perc_red = int(((before_red - after_red)*100)/before_red)\n",
    "    \n",
    "    return perc_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allReductionPercentage(numbers):\n",
    "    '''\n",
    "    takes as input a list of numbers and compure the sorted percentage reduction\n",
    "    between consecutive values as a list. The firts element will be always 0\n",
    "    \n",
    "    '''\n",
    "    reduction = [0]\n",
    "    \n",
    "    for i in range(1,len(numbers)):\n",
    "        j = i-1\n",
    "        reduction.append(reductionPercentage(numbers[j], numbers[i]))\n",
    "        \n",
    "    return reduction;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportionWithUnfiltered(unfiltered_value, value_to_compare):\n",
    "    \n",
    "    '''\n",
    "    it takes as input a value after reduction and a value before reduction and\n",
    "    computes the proportion with the value of unfiltered terms \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return int(((unfiltered_value-value_to_compare)/unfiltered_value)*100)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the table of reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distinct_Terms = {\"Type\": [\"Unfiltered\", \"No numbers\", \"Terms without symbols\", \"Case folding\", \"30 Stop Words\", \"150 Stop Words\", \"Stemming\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_terms, tokenized_terms_per_doc = createTokens(corpus_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_terms, distinct_terms_per_doc = distinctTerms(tokenized_terms_per_doc, tokenized_terms)\n",
    "unfiltered_value = len(distinct_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms without numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numbers = TermsNoNum(distinct_terms)\n",
    "no_numbers_value = len(no_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms without symbols (except for combined words e.g. \"first-degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_alpha_terms = keepOnlyAlph(no_numbers)\n",
    "only_alpha_terms_value = len(only_alpha_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms after case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_fold = list(set(caseFolding(no_numbers)))\n",
    "case_fold_value = len(case_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms after removing 30 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_30 = removeStopWords(case_fold, 30)\n",
    "stop_words_30_value = len(stop_words_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms after removing 150 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_150 = removeStopWords(case_fold, 150)\n",
    "stop_words_150_value = len(stop_words_150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct terms after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = list(set(stemming(stop_words_150)))\n",
    "stemmed_value = len(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_filtered = [unfiltered_value, no_numbers_value, only_alpha_terms_value ,case_fold_value, \n",
    "                   stop_words_30_value, stop_words_150_value, stemmed_value]\n",
    "\n",
    "\n",
    "reduction_filtered = allReductionPercentage(number_filtered)\n",
    "\n",
    "proportion_with_unfiltered = [0]\n",
    "for value in number_filtered[1:]:\n",
    "    proportion_with_unfiltered.append(proportionWithUnfiltered(unfiltered_value, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Reduction %</th>\n",
       "      <th>Proportion with unfiltered %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfiltered</td>\n",
       "      <td>47305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No numbers</td>\n",
       "      <td>43687</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terms without symbols</td>\n",
       "      <td>42554</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case folding</td>\n",
       "      <td>39154</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30 Stop Words</td>\n",
       "      <td>39131</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150 Stop Words</td>\n",
       "      <td>39023</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stemming</td>\n",
       "      <td>28132</td>\n",
       "      <td>27</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Type  Numbers  Reduction %  Proportion with unfiltered %\n",
       "0             Unfiltered    47305            0                             0\n",
       "1             No numbers    43687            7                             7\n",
       "2  Terms without symbols    42554            2                            10\n",
       "3           Case folding    39154            7                            17\n",
       "4          30 Stop Words    39131            0                            17\n",
       "5         150 Stop Words    39023            0                            17\n",
       "6               Stemming    28132           27                            40"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Distinct_Terms['Numbers'] = number_filtered\n",
    "Distinct_Terms['Reduction %'] = reduction_filtered\n",
    "Distinct_Terms['Proportion with unfiltered %'] = proportion_with_unfiltered\n",
    "\n",
    "table_distinct_terms = DataFrame(Distinct_Terms)\n",
    "table_distinct_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 59845),\n",
       " (',', 51208),\n",
       " ('.', 43369),\n",
       " ('of', 25536),\n",
       " ('to', 24047),\n",
       " ('a', 21279),\n",
       " ('and', 20697),\n",
       " ('in', 20661),\n",
       " ('said', 12851),\n",
       " ('``', 11264)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency(caseFolding(tokenized_terms)).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issues we faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems we have faced was dealing with the words that include a number, such as \"10-years-old\". The issue came when we tried to remove the numbers. We have realized that some Python commands remove such phrases completely.\n",
    "\n",
    "Removing symbols caused a problem similar to the previous one. Some words are made of more than one part and “-” is used between the parts. The result of erasing symbols was losing some words such as “first-degree”.\n",
    "\n",
    "Another issue with removing numbers was finding out the symbol for decimal and Milhar number (it can be \",\" or \".\", or\"/\").\n",
    "\n",
    "In the beginning, we structured the case-folding function in a way that it will leave just the strings with all alphabetical characters in them. However, it would have been removed also some terms that we didn't remove before. This means that the number we would have obtained would have been misleading. As a result we created the \"keepOnlyAlph\" function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
