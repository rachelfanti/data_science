{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "### Company Bankruptcy Prediction\n",
    "\n",
    "Names:    \n",
    "    - Denis Mugisha   \n",
    "    - Liu Guangqiang   \n",
    "    - Rachel Fanti   \n",
    "    \n",
    "Dataset: https://www.kaggle.com/fedesoriano/company-bankruptcy-prediction\n",
    "\n",
    "Data: May/2021\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, auc\n",
    "from sklearn.model_selection import learning_curve, validation_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix:**  \n",
    "- True positives: positive tuples correctly labeled   \n",
    "- True negatives: negative tuples correctly labeled   \n",
    "- False positives: negative tuples incorrectly labeled   \n",
    "- False negatives: positive tuples incorrectly labeled   \n",
    "\n",
    "**Classification report:**   \n",
    "- Accuracy = (TP+TN)/(TP+TN+FP+FN)   \n",
    "- Precision = TP/(TP+FP)   \n",
    "- Recall = TP/(TP+FN)   \n",
    "- F1 - measure = 2rp/(r+p)   \n",
    "- WeightedAccuracy = (Wtp*TP+Wtn*TN)/(Wtp*TP+Wtn*TN+Wfp*FP+Wfn*FN)  \n",
    "\n",
    "**Error:**\n",
    "- Mean squared error (MSE)\n",
    "\n",
    "**Receiver Operating Characteristic (ROC) curve:** \n",
    " - Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame(columns = ['Scenarios','#Features', 'Train/Test', 'Acc', 'Bal_Acc','M_P','M_R', 'M_F1', \"P0\", \"P1\", \"R0\", \"R1\", 'MSE', 'Auc'])\n",
    "# print (df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_set(clf, y, predict, df_results, type_set, scenario_name, n_feat):\n",
    "    \n",
    "    '''Calculate metrics given a scenario and set (train/validatioon or test set)'''\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y, predict, output_dict=True)\n",
    "    accuracy = round(report['accuracy'], 2)\n",
    "    balanced_accuracy = round(balanced_accuracy_score(y, predict), 2)\n",
    "    macro_precision =  round(report['macro avg']['precision'],2)\n",
    "    macro_recall = round(report['macro avg']['recall'], 2)    \n",
    "    macro_f1 = round(report['macro avg']['f1-score'], 2)\n",
    "    p0 = round(report['0']['precision'], 2)\n",
    "    p1 = round(report['1']['precision'], 2)\n",
    "    r0 = round(report['0']['recall'], 2)\n",
    "    r1 = round(report['1']['recall'], 2) \n",
    "    \n",
    "    # MSE - Mean squared error\n",
    "    mse = round(mean_squared_error (y,predict),3)\n",
    "    \n",
    "    # AUC\n",
    "    fpr, tpr, thresh = roc_curve(y, predict)\n",
    "    area = round(auc(fpr, tpr),2)\n",
    "    \n",
    "    df_results.loc[len(df_results)+1] = [scenario_name, str(n_feat) + ' features', type_set, accuracy, balanced_accuracy, \n",
    "                                         macro_precision, macro_recall, macro_f1, p0, p1, r0, r1, mse, area]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_scenario(clf, y_train, y_test, predict_train, predict_test, df_results, scenario_name, n_feat):\n",
    "    evaluation_set(clf, y_train, predict_train, df_results,'Train', scenario_name, n_feat)\n",
    "    evaluation_set(clf, y_test, predict_test, df_results,'Val/Test', scenario_name, n_feat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(clf, X, y, type_set):\n",
    "    \n",
    "    # Confusion matrix\n",
    "\n",
    "    print(f'Confusion matrix - {type_set}:')\n",
    "    plot_confusion_matrix(clf, X, y)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(clf, y, predict, type_set):\n",
    "    \n",
    "    print(f'Classification Report - {type_set}:')\n",
    "    print(classification_report(y, predict))\n",
    "    print()       \n",
    "    print ('MSE - {type_set}:', round(mean_squared_error (y,predict),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y, predict, type_set):\n",
    "\n",
    "    fpr, tpr, thresh = roc_curve(y, predict)\n",
    "    area = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %.2f)' %area)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')\n",
    "    plt.title(f'AUC & ROC curve - {type_set}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_scenario(clf, X_train, y_train, X_test, y_test, predict_train, predict_test):\n",
    "    \n",
    "    # print_confusion_matrix(clf, X_train, y_train, 'Train')\n",
    "    conf_matrix = print_confusion_matrix(clf, X_test, y_test, 'Val/Test')\n",
    "    # print_classification_report(clf, y_train, predict_train, 'Train')\n",
    "    # print_classification_report(clf, y_test, predict_test, 'Val/Test')\n",
    "    # plot_roc_curve(y_train, predict_train, 'Train')\n",
    "    roc = plot_roc_curve(y_test, predict_test, 'Val/Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning curve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, ylim=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    #cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=None, shuffle=False) # cv = 5, instead of 10 to avoid samples without class 1  \n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, scoring='balanced_accuracy', cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "    print(f'#Cv - Train: {train_scores.shape} - Test: {test_scores.shape}\\n')\n",
    "    print('Train and test scores according to the sample size\\n')\n",
    "    print('Train: {0}'.format(train_scores_mean))\n",
    "    print('Test:  {0}'.format(test_scores_mean)) \n",
    "    \n",
    "    title = \"Learning Curve for Multi Layer Preceptron (MLP)\\n\" \\\n",
    "        \"Cross Validation of {cv} splits\\n\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Number of training examples\")\n",
    "    plt.ylabel(\"Balanced Accuracy Score\")\n",
    "          \n",
    "    #plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Testing score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
