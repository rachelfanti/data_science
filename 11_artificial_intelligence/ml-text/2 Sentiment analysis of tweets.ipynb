{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of Tweets\n",
    "\n",
    "The directory `data/Tweets` contains a dataset of tweets by airlines customers. Tweets can contain positive (Label=1) or negative (Label=0) comments. \n",
    "\n",
    "The `train.csv` contains annotated data, while `test.csv` includes only the textual data.\n",
    "\n",
    "Your task is to train a classifier to predict the class of the tweets in `test.csv`, you should generate a comma separated file `tweet_results.csv` with a **single column** named `Label` containing the predicted class of the corresponding tweet.\n",
    "\n",
    "The test data was originally annotated and the accuracy of your prediction will be evaluated w.r.t. this ground truth.\n",
    "\n",
    "This notebook should contain all the description of your experiments, the code to generate the classifier **and** the result file, as well as the rationale for your choices. If you prefer to split your work in different source files and/or notebooks, then use this notebook as a guide to the rest of your submitted material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your code below\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\samue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install wordsegment\n",
    "# pip install gensim\n",
    "\n",
    "# Basics\n",
    "import collections, itertools, joblib, re\n",
    "import gensim as gs\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from wordsegment import load, segment\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Models and Evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "\n",
    "# pip install imblearn\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Files\n",
    "import json\n",
    "# Text\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets\n",
      "Train: 8007 Test: 890\n",
      "\n",
      "Tweets for training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@AmericanAir well Done all of you xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@united is the worst airline. Lost my luggage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@united Why don't you respond to my e-mails of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir our flight AA 1338 is delayed out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@virginAmerica Other carriers are less than ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways you guys have my luggage in San Jos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways #usairways lost a passenger today f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@united OMG!!! you just bumped me off the last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>@SouthwestAir Finally! Integration w/ passbook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir stranded in Miami because your au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               text\n",
       "0      1               @AmericanAir well Done all of you xx\n",
       "1      0  @united is the worst airline. Lost my luggage ...\n",
       "2      0  @united Why don't you respond to my e-mails of...\n",
       "3      0  @AmericanAir our flight AA 1338 is delayed out...\n",
       "4      0  @virginAmerica Other carriers are less than ha...\n",
       "5      0  @USAirways you guys have my luggage in San Jos...\n",
       "6      0  @USAirways #usairways lost a passenger today f...\n",
       "7      0  @united OMG!!! you just bumped me off the last...\n",
       "8      1  @SouthwestAir Finally! Integration w/ passbook...\n",
       "9      0  @AmericanAir stranded in Miami because your au..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data for training and testing and generate dataframes\n",
    "\n",
    "tweets_train_path = r'.\\data\\Tweets\\train.csv'\n",
    "tweets_test_path = r'.\\data\\Tweets\\test.csv'\n",
    "\n",
    "tweets_train_df = pd.read_csv(tweets_train_path, header=0)\n",
    "tweets_test_df = pd.read_csv(tweets_test_path, header=0)\n",
    "\n",
    "print('Number of tweets\\nTrain:', len(\n",
    "    tweets_train_df), 'Test:', len(tweets_test_df))\n",
    "print('\\nTweets for training')\n",
    "tweets_train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "1. Cleaning\n",
    "2. Tokenize\n",
    "3. Split words that are together (e.g. *datascience -> data science*)\n",
    "4. Removing stop words \n",
    "5. Omitting terms with length lower than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(tweet):\n",
    "    '''\n",
    "    1. Remove HTTP/HTTPS and URLs\n",
    "    2. Remove mentions\n",
    "    3. Remove hashtag symbols\n",
    "    4. Split words \n",
    "    5. Remove digits\n",
    "    6. Convert text to lower case\n",
    "'''\n",
    "\n",
    "    # to remove links that start with HTTP/HTTPS in the tweet\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "\n",
    "    # to remove other url links\n",
    "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', ' ',\n",
    "                   tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # remove mentions\n",
    "    tweet = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "\n",
    "    # remove hashtags symbols\n",
    "    tweet = re.sub(r\"#\", \"\", tweet)\n",
    "\n",
    "    # to split words that are together\n",
    "    tweet = ''.join([a for a in re.split('([A-Z][a-z]+)', tweet) if a])\n",
    "\n",
    "    # to remove digits\n",
    "    tweet = re.sub(r\"\\d\", \"\", tweet)\n",
    "\n",
    "    # to lower the characters in a text\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokens(tweet):\n",
    "    '''\n",
    "    Given a tweet as input, it returns list with all tokens\n",
    "    '''\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    return tknzr.tokenize(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWordsLemma(tokens):\n",
    "    '''\n",
    "    Given a list of tokens it divides words that are together (e.g. datascience -> data science)\n",
    "    and lemmatize words: \"rocks\" -> \"rock\", \"corpora\" -> corpus\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    splitted = []\n",
    "\n",
    "    for term in tokens:\n",
    "        term = ' '.join(segment(term))\n",
    "        splitted.append(term)\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "    splitted_tokens = [tknzr.tokenize(st) for st in splitted]\n",
    "    tokens = []\n",
    "\n",
    "    for doc_tokens in splitted_tokens:\n",
    "        for word in doc_tokens:\n",
    "            word_lemma = lemmatizer.lemmatize(word)\n",
    "            tokens.append(word_lemma)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(terms):\n",
    "    '''\n",
    "    Given as input a list of terms and the number of stop words to be removed, \n",
    "    it gives a list of terms removing those stop words.\n",
    "    '''\n",
    "    terms_no_stopwords = []\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    for term in terms:\n",
    "        if term not in stop_words:\n",
    "            terms_no_stopwords.append(term)\n",
    "\n",
    "    return terms_no_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omittingShortTerms(terms):\n",
    "    '''\n",
    "    if the words are short this will be omitted from the text.\n",
    "    '''\n",
    "\n",
    "    # the stemmer requires a language parameter\n",
    "\n",
    "    terms_kept = []\n",
    "\n",
    "    for term in terms:\n",
    "        if len(term) >= 3:\n",
    "            terms_kept.append(term)\n",
    "\n",
    "    return terms_kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tweet):\n",
    "    '''\n",
    "    given a document it preprocess it following these steps:\n",
    "        - tokenize\n",
    "        - split words\n",
    "        - lemmatizer\n",
    "        - remove stop words\n",
    "        - omitting short terms\n",
    "    '''\n",
    "    final = []\n",
    "\n",
    "    # Cleaning\n",
    "    cleaned_tweet = cleaning(tweet)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = createTokens(cleaned_tweet)\n",
    "\n",
    "    # Split words and lemmatize\n",
    "    split = splitWordsLemma(tokens)\n",
    "\n",
    "    # remove stop words\n",
    "    no_sw = removeStopWords(split)\n",
    "\n",
    "    # omitting terms with lenght = 1\n",
    "    final = omittingShortTerms(no_sw)\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "load()\n",
    "terms_train = [preprocessing(x) for x in tweets_train_df['text']]\n",
    "terms_test = [preprocessing(x) for x in tweets_test_df['text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files, in case it is necessary\n",
    "with open(\"vocabulary/vocabulary_train_tweeter\", \"w\") as fp:\n",
    "    json.dump(terms_train, fp)\n",
    "\n",
    "with open(\"vocabulary/vocabulary_test_tweeter\", \"w\") as fp:\n",
    "    json.dump(terms_test, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading files and summmaryzing the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val - We have 8007 documents and 6325 terms in our collection.\n",
      "Test - We have 890 documents and 2064 terms in our collection.\n"
     ]
    }
   ],
   "source": [
    "with open(\"vocabulary/vocabulary_train_tweeter\", \"r\") as fp:\n",
    "    terms_train = json.load(fp)\n",
    "\n",
    "with open(\"vocabulary/vocabulary_test_tweeter\", \"r\") as fp:\n",
    "    terms_test = json.load(fp)\n",
    "\n",
    "detokenizer = Detok()\n",
    "sentences_train = [detokenizer.detokenize(\n",
    "    terms_doc) for terms_doc in terms_train]\n",
    "sentences_test = [detokenizer.detokenize(\n",
    "    terms_doc) for terms_doc in terms_test]\n",
    "\n",
    "dct_train = Dictionary(terms_train)\n",
    "dct_test = Dictionary(terms_test)\n",
    "print('Train/Val - We have '+str(dct_train.num_docs)+' documents and ' +\n",
    "      str(len(dct_train.token2id))+' terms in our collection.')\n",
    "print('Test - We have '+str(dct_test.num_docs)+' documents and ' +\n",
    "      str(len(dct_test.token2id))+' terms in our collection.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight', 3441),\n",
       " ('hour', 947),\n",
       " ('service', 880),\n",
       " ('customer', 819),\n",
       " ('get', 813)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of all words across tweets for training\n",
    "words_in_tweets = list(itertools.chain(*terms_train))\n",
    "# Create counter\n",
    "words_freq = collections.Counter(words_in_tweets)\n",
    "words_freq.most_common(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix_shape:\n",
      "Train: (8007, 6325)  Test: (890, 6325)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "doc_term_train = vectorizer.fit_transform(sentences_train) #fit and transform\n",
    "doc_term_test = vectorizer.transform(sentences_test) #test set only transform\n",
    "print('Document-term matrix_shape:\\nTrain:',doc_term_train.shape,' Test:', doc_term_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain the dataset for training, validating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows/Labels\n",
      "Train: 8007 = 8007\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = doc_term_train, doc_term_test\n",
    "y_train = tweets_train_df['Label']\n",
    "\n",
    "print('Number of rows/Labels\\nTrain:', X_train.shape[0], '=', len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows/Labels\n",
      "Train: 6005 = 6005\n",
      "Val: 2002 = 2002\n"
     ]
    }
   ],
   "source": [
    "# These data will be used in cases when it is necessary to adjust the hypermarameters\n",
    "X_train_75, X_val_25, y_train_75, y_val_25 = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print('Number of rows/Labels\\nTrain:', (X_train_75.shape[0]), '=', len(y_train_75))\n",
    "print('Val:', (X_val_25.shape[0]), '=', len(y_val_25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Classifier - KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.17 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.835803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.869942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.869276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.858618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.869942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_neighbors  Accuracy\n",
       "0            1  0.835803\n",
       "1            3  0.869942\n",
       "2            5  0.869276\n",
       "3            7  0.858618\n",
       "4           10  0.869942"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the classifier\n",
    "base_estimator = KNeighborsClassifier(algorithm='brute')\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 10]}\n",
    "clf = GridSearchCV(base_estimator, param_grid, cv=5)\n",
    "\n",
    "%time clf.fit(X_train_75, y_train_75)\n",
    "\n",
    "pd.concat([pd.DataFrame(clf.cv_results_[\"params\"]), pd.DataFrame(\n",
    "    clf.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = clf.predict(X_val_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='brute', n_neighbors=3)\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[1562   99]\n",
      " [ 119  222]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      1661\n",
      "           1       0.69      0.65      0.67       341\n",
      "\n",
      "    accuracy                           0.89      2002\n",
      "   macro avg       0.81      0.80      0.80      2002\n",
      "weighted avg       0.89      0.89      0.89      2002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_mat = confusion_matrix(y_val_25, predicted_label)\n",
    "class_report = classification_report(y_val_25, predicted_label)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(\"\\n\\nConfusion Matrix:\\n\")\n",
    "print(confusion_mat)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes of this model are imbalanced, the model tends to learn the most common class (class 0) and it doesn’t abstract information from the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed solutions for imbalanced data:\n",
    "\n",
    "- Stratified train-test split\n",
    "- Stratified k-fold cross-validation\n",
    "- Metric to be included 'balanced accuracy'\n",
    "- Classifiers + Class weight\n",
    "- Re-sampling techniques\n",
    "- Techniques of bagging and boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Unbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary will be used to store the results of the models with 2 scores: accuracy and balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and dictionary to store the results of our analysis\n",
    "index = []\n",
    "scores = {\"Accuracy\": [], \"Balanced accuracy\": []}\n",
    "scoring = [\"accuracy\", \"balanced_accuracy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic classifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to be analysed: Logistic Regression, Decision Forest and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Balanced accuracy\n",
       "Logistic Regression      0.93               0.87\n",
       "Random Forest            0.93               0.87\n",
       "Decision Tree            0.89               0.83"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=1000))\n",
    "rf_clf = make_pipeline(RandomForestClassifier(random_state=42))\n",
    "dt_clf = make_pipeline(DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "for model_name, model in zip(['Logistic Regression', 'Random Forest', 'Decision Tree'], [lr_clf, rf_clf, dt_clf]):\n",
    "    # Cross_validate: for int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.\n",
    "    cv_result = cross_validate(model, X_train, y_train, scoring=scoring)\n",
    "    index.append(model_name)\n",
    "    scores[\"Accuracy\"].append(round(cv_result[\"test_accuracy\"].mean(), 2))\n",
    "    scores[\"Balanced accuracy\"].append(\n",
    "        round(cv_result[\"test_balanced_accuracy\"].mean(), 2))\n",
    "    df_scores = pd.DataFrame(scores, index=index)\n",
    "    joblib.dump(model, (\"models\\_\" + model_name.replace(\" \", \"\") + \"_classifier.joblib\"))\n",
    "\n",
    "df_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifiers + Class weight**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the models in scikit-learn have a parameter class_weight. This parameter will affect the computation of the loss in linear model or the criterion in the tree-based model to penalize differently a false classification from the minority and majority class. Therefore, it will be set class_weight=\"balanced\" such that the weight applied is inversely proportional to the class frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression with balanced class weights</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest with balanced class weights</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision tree with balanced class weights</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Accuracy  Balanced accuracy\n",
       "Logistic Regression                                  0.93               0.87\n",
       "Random Forest                                        0.93               0.87\n",
       "Decision Tree                                        0.89               0.83\n",
       "Logistic regression with balanced class weights      0.92               0.88\n",
       "Random forest with balanced class weights            0.93               0.86\n",
       "Decision tree with balanced class weights            0.87               0.84"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.set_params(logisticregression__class_weight=\"balanced\")\n",
    "rf_clf.set_params(randomforestclassifier__class_weight=\"balanced\")\n",
    "dt_clf.set_params(decisiontreeclassifier__class_weight=\"balanced\")\n",
    "\n",
    "for model_name, model in zip(['Logistic regression with balanced class weights', 'Random forest with balanced class weights', 'Decision tree with balanced class weights'], [lr_clf, rf_clf, dt_clf]):\n",
    "    # Cross_validate: for int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.\n",
    "    cv_result = cross_validate(model, X_train, y_train, scoring=scoring)\n",
    "    index.append(model_name)\n",
    "    scores[\"Accuracy\"].append(round(cv_result[\"test_accuracy\"].mean(), 2))\n",
    "    scores[\"Balanced accuracy\"].append(\n",
    "        round(cv_result[\"test_balanced_accuracy\"].mean(), 2))\n",
    "    df_scores = pd.DataFrame(scores, index=index)\n",
    "    joblib.dump(model, (\"models\\_\" + model_name.replace(\" \", \"\") + \"_w_classifier.joblib\"))\n",
    "df_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resample the training set during learning** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of addressing imbalanced data is by re-sampling the dataset as to offset this imbalance.   \n",
    "It consists of removing samples from the majority class (under-sampling) or adding more examples from the minority class (over-sampling).   \n",
    "However, the implementation of over-sampling duplicates random records from the minority class, which can cause overfitting.\n",
    "Using under-sampling, removing random records from the majority class, wich can cause loss of information.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression with balanced class weights</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest with balanced class weights</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision tree with balanced class weights</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under-sampling + Logistic regression</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under-sampling + Random forest</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under-sampling + Decision Tree</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Accuracy  Balanced accuracy\n",
       "Logistic Regression                                  0.93               0.87\n",
       "Random Forest                                        0.93               0.87\n",
       "Decision Tree                                        0.89               0.83\n",
       "Logistic regression with balanced class weights      0.92               0.88\n",
       "Random forest with balanced class weights            0.93               0.86\n",
       "Decision tree with balanced class weights            0.87               0.84\n",
       "Under-sampling + Logistic regression                 0.91               0.91\n",
       "Under-sampling + Random forest                       0.88               0.89\n",
       "Under-sampling + Decision Tree                       0.80               0.83"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_u = make_pipeline_with_sampler(RandomUnderSampler(\n",
    "    random_state=42), LogisticRegression(max_iter=1000),)\n",
    "rf_clf_u = make_pipeline_with_sampler(RandomUnderSampler(\n",
    "    random_state=42), RandomForestClassifier(random_state=42, n_jobs=2),)\n",
    "dt_clf_u = make_pipeline_with_sampler(RandomUnderSampler(\n",
    "    random_state=42), DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "for model_name, model in zip(['Under-sampling + Logistic regression', 'Under-sampling + Random forest', 'Under-sampling + Decision Tree'], [lr_clf_u, rf_clf_u, dt_clf_u]):\n",
    "    # Cross_validate: for int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.\n",
    "    cv_result = cross_validate(model, X_train, y_train, scoring=scoring)\n",
    "    index.append(model_name)\n",
    "    scores[\"Accuracy\"].append(round(cv_result[\"test_accuracy\"].mean(), 2))\n",
    "    scores[\"Balanced accuracy\"].append(\n",
    "        round(cv_result[\"test_balanced_accuracy\"].mean(), 2))\n",
    "    df_scores = pd.DataFrame(scores, index=index)\n",
    "    joblib.dump(model, (\"models\\_\" + model_name.replace(\" \", \"\") + \"_u_classifier.joblib\"))\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use of specific balanced algorithms from imbalanced-learn, bagging and boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree models of bagging and bossting will be used:\n",
    "\n",
    "- Balanced random forest: randomly under-samples each bootstrap sample to balance it.\n",
    "- Histogram-based Gradient Boosting Classification Tree: similar to gradient boosting trees, reduces the number of splitting points to consider, faster than Gradient Boosting Classifier\n",
    "- BalancedBaggingClassifier:  Bagging, it includes an additional step to balance the training set at fit time using a given sampler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression with balanced class weights</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest with balanced class weights</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision tree with balanced class weights</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under-sampling + Logistic regression</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under-sampling + Random forest</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under-sampling + Decision Tree</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced random forest</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced bag of histogram gradient boosting</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Accuracy  Balanced accuracy\n",
       "Logistic Regression                                  0.93               0.87\n",
       "Random Forest                                        0.93               0.87\n",
       "Decision Tree                                        0.89               0.83\n",
       "Logistic regression with balanced class weights      0.92               0.88\n",
       "Random forest with balanced class weights            0.93               0.86\n",
       "Decision tree with balanced class weights            0.87               0.84\n",
       "Under-sampling + Logistic regression                 0.91               0.91\n",
       "Under-sampling + Random forest                       0.88               0.89\n",
       "Under-sampling + Decision Tree                       0.80               0.83\n",
       "Balanced random forest                               0.88               0.90\n",
       "Balanced bag of histogram gradient boosting          0.89               0.89"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "rf_clf_b = make_pipeline(BalancedRandomForestClassifier(random_state=42, n_jobs=2),)\n",
    "bag_clf = make_pipeline(BalancedBaggingClassifier(base_estimator=HistGradientBoostingClassifier(\n",
    "    random_state=42), n_estimators=10, random_state=42, n_jobs=2,),)\n",
    "\n",
    "for model_name, model in zip(['Balanced random forest', 'Balanced bag of histogram gradient boosting', 'Balanced bag of histogram gradient boosting'], [rf_clf_b, bag_clf]):\n",
    "    # Cross_validate: for int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used.\n",
    "    cv_result = cross_validate(model, X_train.toarray(), y_train, scoring=scoring)\n",
    "    index.append(model_name)\n",
    "    scores[\"Accuracy\"].append(round(cv_result[\"test_accuracy\"].mean(), 2))\n",
    "    scores[\"Balanced accuracy\"].append(\n",
    "        round(cv_result[\"test_balanced_accuracy\"].mean(), 2))\n",
    "    df_scores = pd.DataFrame(scores, index=index)\n",
    "    joblib.dump(model, (\"models\\_\" + model_name.replace(\" \", \"\") + \"_classifier.joblib\"))\n",
    "df_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = joblib.load('models\\_RandomForest_classifier.joblib')\n",
    "rf_bal = joblib.load('models\\_Balancedrandomforest_classifier.joblib')\n",
    "lr_u =  joblib.load('models\\_Under-sampling+Logisticregression_u_classifier.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomundersampler', RandomUnderSampler(random_state=42)),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "rf_bal.fit(X_train, y_train)\n",
    "lr_u.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(890, 6325) 890 890 890\n"
     ]
    }
   ],
   "source": [
    "y_predict_rf = rf.predict(X_test)\n",
    "y_predict_rf_bal = rf_bal.predict(X_test)\n",
    "y_predict_lr_u = lr_u.predict(X_test)\n",
    "print (X_test.shape, len (y_predict_rf), len(y_predict_rf_bal), len (y_predict_lr_u ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_rf = pd.DataFrame()\n",
    "df_results_rf['Label'] = y_predict_rf\n",
    "df_results_rf.to_csv('tweet_results_rf.csv', index=False)\n",
    "\n",
    "df_results_rf_bal = pd.DataFrame()\n",
    "df_results_rf_bal['Label'] = y_predict_rf_bal\n",
    "df_results_rf_bal.to_csv('tweet_results_rf_bal.csv', index=False)\n",
    "\n",
    "df_results_lr_u = pd.DataFrame()\n",
    "df_results_lr_u['Label'] = y_predict_lr_u\n",
    "df_results_lr_u.to_csv('tweet_results_lr_u.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, We selected basic Random Forest as the mains solution because undersampling can remove important information from the data.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f78d7e7361514bf5fc2f1790832692eb3775ade82ec96c5e470dd51518d94d05"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('aima2021')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
