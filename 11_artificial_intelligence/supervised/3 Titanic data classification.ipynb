{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Predicting the Survival of Titanic Passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we consider the [Kaggle](https://www.kaggle.com) competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "The description of the problem is (from the Kaggle page):\n",
    "\n",
    "> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    ">\n",
    "> One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    ">\n",
    "> In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    "The description of the data is available on the [Kaggle page](https://www.kaggle.com/c/titanic/data), while the actual data is available in the directory `data/titanic/`.\n",
    "\n",
    "Below the description taken from the website:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Overview\n",
    "> \n",
    "> The data has been split into two groups:\n",
    "> \n",
    "> -   training set (train.csv)\n",
    "> -   test set (test.csv)\n",
    "> \n",
    "> The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \"ground truth\") for each passenger. Your model will be based on \"features\" like passengers' gender and class. You can also use [feature engineering ](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/) to create new features.\n",
    "> \n",
    "> The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "> \n",
    "> We also include `gender_submission.csv`, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n",
    "> \n",
    "> ### Data Dictionary\n",
    "> \n",
    "> | Variable | Definition | Key |\n",
    "> |----------|------------|-----|\n",
    "> | survival | Survival | 0 = No, 1 = Yes |\n",
    "> | pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "> | sex | Sex |  |\n",
    "> | Age | Age in years |  |\n",
    "> | sibsp | # of siblings / spouses aboard the Titanic |  |\n",
    "> | parch | # of parents / children aboard the Titanic |  |\n",
    "> | ticket | Ticket number |  |\n",
    "> | fare | Passenger fare |  |\n",
    "> | cabin | Cabin number |  |\n",
    "> | embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "> \n",
    "> ### Variable Notes\n",
    "> \n",
    "> **pclass**: A proxy for socio-economic status (SES)  \n",
    "> 1st = Upper  \n",
    "> 2nd = Middle   \n",
    "> 3rd = Lower\n",
    "> \n",
    "> **age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "> \n",
    "> **sibsp**: The dataset defines family relations in this way...  \n",
    "> Sibling = brother, sister, stepbrother, stepsister  \n",
    "> Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "> \n",
    "> **parch**: The dataset defines family relations in this way...  \n",
    "> Parent = mother, father  \n",
    "> Child = daughter, son, stepdaughter, stepson  \n",
    "> Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating your solution\n",
    "\n",
    "You should adopt the same submission rules as the [Kaggle competition](https://www.kaggle.com/c/titanic#evaluation):\n",
    "\n",
    "> **Goal**\n",
    "> \n",
    "> It is your job to predict if a passenger survived the sinking of the Titanic or not. \n",
    "> For each PassengerId in the test set, you must predict a 0 or 1 value for the Survived variable.\n",
    "> \n",
    "> **Metric**\n",
    ">\n",
    "> Your score is the percentage of passengers you correctly predict. This is known simply as \"accuracy”.\n",
    "> \n",
    "> **Submission File Format**\n",
    "> \n",
    "> You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n",
    "> \n",
    "> The file should have exactly 2 columns:\n",
    "> \n",
    "> ```\n",
    "> PassengerId (sorted in any order)\n",
    "> Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n",
    "> PassengerId,Survived\n",
    ">  892,0\n",
    ">  893,1\n",
    ">  894,0\n",
    ">  Etc.\n",
    "> ```\n",
    "> \n",
    "> You can download an example submission file (gender_submission.csv) on the Data page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use the techniques learned in the previous exercises to build a classifier, using the *training set*, that provides the survival prediction for the *test set*.\n",
    "\n",
    "Create a file `titanic-submission.csv` in the current directory with the prediction. You can also provide different predictions (use `_n.csv`) to distinguish them, but you need to describe how they have been generated. To export to CSV you can use the pandas [pandas.DataFrame.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) method.\n",
    "\n",
    "It is paramount to explain the reason for your choices, the rationale behind your approach and decisions is more important than the final result. The explanation must be included in this notebook.\n",
    "\n",
    "You can also register in [Kaggle](https://www.kaggle.com) and submit your solution to get the accuracy of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes\n",
    "\n",
    "- ML algorithms work better with numerical values, you should cast categorical features into integers.\n",
    "- For classification, continuous numerical values may lead to overfitting, it's a common practice to aggregate the values. Pandas provides an handy [quantile-based discretization function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html).\n",
    "- Too many features may lead to overfitting, better drop the least significant ones.\n",
    "- Data visualisation may help in identifying relevant features.\n",
    "- You may want to create new features based on existing ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your code below\n",
    "\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
